{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#import torchtext\n",
    "#from torchtext.datasets import text_classification\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import random_split\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 把为空的评论样本删除掉，存入train_sample.csv\n",
    "# data = pd.read_csv('C:\\\\Users\\\\sheng\\\\Desktop\\\\Coronavirus\\\\sample_comment_seg.csv') \n",
    "# data_new = pd.DataFrame({ \"comment_seg\":[], \"score\":[]})\n",
    "# for i in range(len(data)):\n",
    "#     # data.loc[i,\"comment_seg\"].split()\n",
    "#     if str(data.loc[i,\"comment_seg\"]) == \"nan\":\n",
    "#         print(\"yes\")\n",
    "#     else:\n",
    "#         data_new = data_new.append(data.loc[i,])\n",
    "# data_new = data_new[[\"comment_seg\", \"score\"]]\n",
    "# data_new.to_csv('./train_sample.csv', sep=',', header=True, index=True, encoding='utf-8-sig')\n",
    "\n",
    "# data[\"comment_length\"] = None\n",
    "# data[\"label\"] = None\n",
    "# label_dic = {-1: 2, 0: 0, 1: 1}\n",
    "\n",
    "# for i in range(len(data)):\n",
    "#     data.loc[i, \"comment_length\"] = len(data.loc[i,\"comment_seg\"].split())\n",
    "#     data.loc[i, \"label\"] = label_dic.get(data.loc[i, \"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = 'all_comments_seg_jieba.txt'\n",
    "model_word2Vec = Word2Vec(LineSentence(inp), size=embed_dim, window=5, min_count=5, workers=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>comment_seg</th>\n",
       "      <th>score</th>\n",
       "      <th>comment_length</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>求求 那些 刷 车牌 的 能 不能 先 去 国务院 下面 的 疫情 监督 平台 先 去 举报...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>任命 日报 就 可以 不 标明 出处</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>回形针 牛 逼</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>二流 报纸 竟然 发 盗版 视频 啦</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>我 在 武汉 看到 一只 鄂鱼 它 一声 尖叫 它 是 号 米 长 它 可恶 的 状 出入 ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  \\\n",
       "0           0             0   \n",
       "1           1             1   \n",
       "2           2             2   \n",
       "3           3             3   \n",
       "4           4             4   \n",
       "\n",
       "                                         comment_seg  score  comment_length  \\\n",
       "0  求求 那些 刷 车牌 的 能 不能 先 去 国务院 下面 的 疫情 监督 平台 先 去 举报...    1.0              72   \n",
       "1                                 任命 日报 就 可以 不 标明 出处   -1.0               7   \n",
       "2                                            回形针 牛 逼    1.0               3   \n",
       "3                                 二流 报纸 竟然 发 盗版 视频 啦   -1.0               7   \n",
       "4  我 在 武汉 看到 一只 鄂鱼 它 一声 尖叫 它 是 号 米 长 它 可恶 的 状 出入 ...   -1.0              28   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      2  \n",
       "2      1  \n",
       "3      2  \n",
       "4      2  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train_sample.csv') \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2tensor(word):\n",
    "    if word in model_word2Vec.wv.vocab.keys():\n",
    "        v = torch.from_numpy(model_word2Vec.wv[word])\n",
    "    else:\n",
    "        # v = torch.randn(embed_dim)\n",
    "        v = torch.zeros(embed_dim)\n",
    "    return(v)\n",
    "def comment_seg2tensor(comment_seg):\n",
    "    comment_seg = comment_seg.split()\n",
    "    t = torch.stack([word2tensor(word) for word in comment_seg])\n",
    "    return(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EMBED_DIM = embed_dim\n",
    "HIDDEN_DIM = 16\n",
    "OUTPUT_DIM = 3\n",
    "N_LAYERS = 1\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetLoader(torch.utils.data.Dataset):\n",
    "    # 初始化函数，得到数据\n",
    "    def __init__(self, comment_seg, comment_length, label):\n",
    "        self.comment_seg = comment_seg\n",
    "        self.comment_length = comment_length\n",
    "        self.label = label\n",
    "        self.len = len(comment_seg)\n",
    "    # index是根据batchsize划分数据后得到的索引，最后将data和对应的labels进行一起返回\n",
    "    def __getitem__(self, index):\n",
    "        return self.comment_seg[index], self.comment_length[index],self.label[index]\n",
    "    # 该函数返回数据大小长度，目的是DataLoader方便划分，如果不知道大小，DataLoader会一脸懵逼\n",
    "    def __len__(self):\n",
    "        return(self.len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    comments = [element[0] for element in batch]\n",
    "    lengths = np.asarray([element[1] for element in batch])\n",
    "    labels = np.asarray([element[2] for element in batch])\n",
    "    MAX_LENGTH = max(lengths)\n",
    "    sort_index = np.argsort(-lengths)\n",
    "    text_tensor = torch.zeros([MAX_LENGTH, BATCH_SIZE, EMBED_DIM], dtype=torch.float)\n",
    "    lengths_sorted = np.zeros(len(comments))\n",
    "    labels_sorted = np.zeros(len(comments))\n",
    "    for i in range(len(lengths)):\n",
    "        text_tensor[0:lengths[sort_index[i]], i,:] = comment_seg2tensor(comments[sort_index[i]])\n",
    "        lengths_sorted[i] = lengths[sort_index[i]] \n",
    "        labels_sorted[i] = labels[sort_index[i]]\n",
    "    return text_tensor, torch.Tensor(lengths_sorted), torch.Tensor(labels_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text_tensor, text_lengths):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        # embedded = self.dropout(self.embedding(text))\n",
    "        embedded = text_tensor\n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        #pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        #unpack sequence\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        # hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)        \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "        predictions = self.fc(hidden)\n",
    "        return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(EMBED_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL,\n",
    "            DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 28,003 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "import time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1.0)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_accuracy(output_, label_):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 8\n",
    "    \"\"\"\n",
    "    acc = (output_.argmax(1) == label_).sum()    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data_, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    c_seg = train_data_[\"comment_seg\"]\n",
    "    c_length =  train_data_[\"comment_length\"]\n",
    "    l = train_data_[\"label\"]\n",
    "    \n",
    "    # 读取数据\n",
    "    train_data_loader = GetLoader(c_seg, c_length, l)\n",
    "    train_data_iterator = DataLoader(train_data_loader, batch_size=BATCH_SIZE, shuffle=True, drop_last=True,collate_fn=generate_batch)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i, batch in enumerate(train_data_iterator):\n",
    "        text_tensor = batch[0].to(device)\n",
    "        text_lengths = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        outputs = model(text_tensor, text_lengths).squeeze(1)\n",
    "        \n",
    "        loss = criterion(outputs, labels.long())\n",
    "        \n",
    "        acc = pred_accuracy(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(train_data_), epoch_acc / len(train_data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix = np.zeros([3,3])\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_data_, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    c_seg = test_data_[\"comment_seg\"]\n",
    "    c_length =  test_data_[\"comment_length\"]\n",
    "    l = test_data_[\"label\"]\n",
    "    \n",
    "    # 读取数据\n",
    "    test_data_loader = GetLoader(c_seg, c_length, l)\n",
    "    test_data_iterator = DataLoader(test_data_loader, batch_size=BATCH_SIZE, shuffle=True, drop_last=True,collate_fn=generate_batch)\n",
    "    \n",
    "    model.eval()\n",
    "    confusion_matrix = np.zeros([3,3])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(test_data_iterator):\n",
    "            text_tensor = batch[0].to(device)\n",
    "            text_lengths = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "            outputs = model(text_tensor, text_lengths).squeeze(1)\n",
    "        \n",
    "            loss = criterion(outputs, labels.long())\n",
    "        \n",
    "            acc = pred_accuracy(outputs, labels)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "            # 计算confusion matrix\n",
    "            labels_pred = outputs.cpu().argmax(1)\n",
    "            for (r,c) in zip(labels_pred, labels.cpu()):\n",
    "                # print(\"r: \", r, \"c: \", c)\n",
    "                confusion_matrix[r.item(), int(c.item())] += 1\n",
    "\n",
    "    return epoch_loss / len(test_data_), epoch_acc / len(test_data_), confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.011 | Train Acc: 71.63%\n",
      "\t Val. Loss: 0.009 |  Val. Acc: 77.66%\n",
      "confusion_matrix:  [[4.800e+01 1.000e+00 1.000e+00]\n",
      " [6.600e+01 9.840e+02 2.340e+02]\n",
      " [3.720e+02 6.110e+02 3.443e+03]]\n",
      "Epoch: 02 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.009 | Train Acc: 78.36%\n",
      "\t Val. Loss: 0.008 |  Val. Acc: 78.71%\n",
      "confusion_matrix:  [[  66.   11.   14.]\n",
      " [  61. 1021.  215.]\n",
      " [ 360.  564. 3448.]]\n",
      "Epoch: 03 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.008 | Train Acc: 79.52%\n",
      "\t Val. Loss: 0.008 |  Val. Acc: 78.84%\n",
      "confusion_matrix:  [[  85.   16.   29.]\n",
      " [  59. 1037.  228.]\n",
      " [ 343.  542. 3421.]]\n",
      "Epoch: 04 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.008 | Train Acc: 80.30%\n",
      "\t Val. Loss: 0.008 |  Val. Acc: 79.26%\n",
      "confusion_matrix:  [[  86.   12.   25.]\n",
      " [  63. 1106.  277.]\n",
      " [ 338.  478. 3375.]]\n",
      "Epoch: 05 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.008 | Train Acc: 81.17%\n",
      "\t Val. Loss: 0.008 |  Val. Acc: 79.45%\n",
      "confusion_matrix:  [[  96.   20.   38.]\n",
      " [  57. 1081.  237.]\n",
      " [ 334.  496. 3401.]]\n",
      "Epoch: 06 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.007 | Train Acc: 81.61%\n",
      "\t Val. Loss: 0.008 |  Val. Acc: 79.40%\n",
      "confusion_matrix:  [[  92.   15.   40.]\n",
      " [  54. 1062.  215.]\n",
      " [ 341.  520. 3421.]]\n",
      "Epoch: 07 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.007 | Train Acc: 81.92%\n",
      "\t Val. Loss: 0.008 |  Val. Acc: 79.56%\n",
      "confusion_matrix:  [[  99.   21.   44.]\n",
      " [  52. 1064.  211.]\n",
      " [ 336.  512. 3421.]]\n",
      "Epoch: 08 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.007 | Train Acc: 82.54%\n",
      "\t Val. Loss: 0.008 |  Val. Acc: 79.09%\n",
      "confusion_matrix:  [[  99.   23.   42.]\n",
      " [  58. 1094.  270.]\n",
      " [ 330.  480. 3364.]]\n",
      "Epoch: 09 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.007 | Train Acc: 83.31%\n",
      "\t Val. Loss: 0.008 |  Val. Acc: 79.04%\n",
      "confusion_matrix:  [[ 125.   34.   77.]\n",
      " [  56. 1087.  257.]\n",
      " [ 306.  476. 3342.]]\n",
      "Epoch: 10 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.007 | Train Acc: 83.55%\n",
      "\t Val. Loss: 0.009 |  Val. Acc: 78.62%\n",
      "confusion_matrix:  [[ 115.   28.   71.]\n",
      " [  68. 1134.  325.]\n",
      " [ 304.  434. 3281.]]\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "train_data_, test_data_, train_, test_ = train_test_split(data, range(len(data)),test_size=0.3, random_state=0)\n",
    "train_data_ = pd.concat([train_data_[\"comment_seg\"], train_data_[\"comment_length\"], train_data_[\"label\"]], axis = 1).reset_index(drop=True)\n",
    "test_data_ = pd.concat([test_data_[\"comment_seg\"], test_data_[\"comment_length\"], test_data_[\"label\"]], axis = 1).reset_index(drop=True)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_data_, optimizer, criterion)\n",
    "    valid_loss, valid_acc, confusion_matrix = test(model, test_data_, optimizer, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "        \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    print(\"confusion_matrix: \", confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 115.,   28.,   71.],\n",
       "       [  68., 1134.,  325.],\n",
       "       [ 304.,  434., 3281.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 487., 1596., 3677.])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(confusion_matrix,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 214., 1527., 4019.])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(confusion_matrix,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6383680555555555"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3677/sum(np.sum(confusion_matrix,axis=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
